# ğŸš€ MIMI Agent â€” Roadmap & Expertenanalyse 2026

> **Erstellt:** 2026-02-10  
> **Status:** Aktive Analyse  
> **Ziel:** MIMI zum SOTA Local-First AI Agent upgraden

---

## ğŸ“Š Aktuelle Diagnose

### âœ… Was schon funktioniert
| Feature | Status | Bewertung |
|---------|--------|-----------|
| Chat (Phi-3.5 mini) | âœ… Funktional | â­â­â­ Gut |
| RAG (Dokumentensuche) | âœ… Funktional | â­â­â­ Gut |
| Python-AusfÃ¼hrung (Pyodide) | âœ… Funktional | â­â­â­â­ Sehr gut |
| Tool-Loop (3 Iterationen) | âœ… Funktional | â­â­â­ Gut |
| Voice Input/Output | âœ… Web Speech API | â­â­ Basis |
| Agent-Orchestrierung | âœ… 10 Spezialisten | â­â­â­ Gut |
| Skills System | âœ… 7 builtin Skills | â­â­â­ Gut |

### ğŸ”´ Vision â€” Diagnose der Probleme

**Aus den Browser-Logs:**
```
dtype not specified for "encoder_model". Using the default dtype (q8) for this device (wasm).
dtype not specified for "decoder_model_merged". Using the default dtype (q8) for this device (wasm).
```

**Problem 1: WASM statt WebGPU**
- Vision-Engine nutzt `@huggingface/transformers` mit WASM-Backend
- WASM ist 30-100x langsamer als WebGPU fÃ¼r Inference
- Kein GPU-Acceleration fÃ¼r die Bildanalyse

**Problem 2: Falsches Modell (vit-gpt2-image-captioning)**
- `Xenova/vit-gpt2-image-captioning` ist ein ~80MB Modell aus 2022
- Generiert nur 1-Satz englische Captions ("a cat sitting on a couch")
- Keine echte BildverstÃ¤ndnis-FÃ¤higkeit
- Kein VQA (Visual Question Answering)
- Kein OCR
- Kein Bounding Box Detection

**Problem 3: Vision-Ergebnis nicht in Chat integriert**
- Vision-Analyse landet nur als `chatHistoryRef.current.push()` Message
- Der LLM (Phi-3.5) bekommt den Vision-Output NICHT als Kontext
- Agent-Routing ignoriert Bild-Kontext (kein Vision-Agent in Klassifikation)
- Das Capability Chip "Bildanalyse" triggert ein Upload, aber der Agent verarbeitet das Ergebnis nicht intelligent

**Problem 4: Keine `analyze_image` Tool-Integration**
- `analyze_image` ist in `executeToolCall()` implementiert (L610-623)
- ABER es fehlt in `TOOL_DEFINITIONS` (wurde auskommentiert, L75-76)
- `setToolContext()` im Engine Hook setzt kein `analyzeImage` (L80-93)
- â†’ Der Agent kann Vision NICHT als Tool aufrufen

---

## ğŸ—ï¸ Verbesserungsplan â€” 4 Phasen

### Phase 1: Vision Fix (Kritisch, Sprint 1) âœ… ABGESCHLOSSEN

#### 1.1 Upgrade auf SmolVLM / Florence-2 (WebGPU) âœ…

**SOTA 2026 Empfehlung:** Ersetze `vit-gpt2-image-captioning` durch ein echtes VLM:

| Modell | GrÃ¶ÃŸe | FÃ¤higkeiten | WebGPU | Browser-Ready |
|--------|-------|-------------|--------|---------------|
| **SmolVLM-Instruct** | ~500MB | VQA, Captioning, Chat Ã¼ber Bilder | âœ… | âœ… Transformers.js |
| **Florence-2-base** | ~450MB | OCR, Captioning, Object Detection, Grounding | âœ… | âœ… Transformers.js |
| **Moondream2** | ~1.6GB | VQA, Captioning, Chat | âœ… | âœ… Experimentell |
| **Phi-3.5-vision** | ~4.2GB | Volles VLM, OCR, Charts, Multi-Frame | âœ… | âœ… WebLLM |

**Empfehlung:** 
- **PrimÃ¤r: SmolVLM-Instruct** â€” Bestes Preis-Leistungs-VerhÃ¤ltnis fÃ¼r Browser
- **Fallback: Florence-2** â€” Robuster, guter OCR, kleinere GrÃ¶ÃŸe
- **Premium-Option: Phi-3.5-vision via WebLLM** â€” Wenn User Premium will

```typescript
// NEU: vision-engine.ts â€” Upgrade
import { pipeline, env } from '@huggingface/transformers';

// WebGPU-First Strategy
env.backends.onnx.wasm.proxy = false;
env.backends.onnx.webgpu = true; // â† CRITICAL: WebGPU aktivieren!

// SmolVLM fÃ¼r echtes BildverstÃ¤ndnis
this.pipeline = await pipeline(
    'image-text-to-text',  // Multimodal Pipeline!
    'HuggingFaceTB/SmolVLM-Instruct',
    { 
        device: 'webgpu',  // GPU-Acceleration!
        dtype: 'q4f16',    // 4-bit Quantization fÃ¼r Speed
    }
);
```

#### 1.2 Vision als Tool in Agent-Loop integrieren âœ…

```typescript
// tool-definitions.ts â€” Tool wieder aktivieren
{
    name: 'analyze_image',
    description: 'Analysiert ein hochgeladenes Bild und beantwortet Fragen dazu',
    parameters: [
        { name: 'question', type: 'string', description: 'Frage zum Bild', required: true }
    ],
    handler: 'analyzeImage'
},

// useMimiEngine.ts â€” Tool Context erweitern
engineRef.current.setToolContext({
    executePython: async (code) => { ... },
    searchDocuments: async (query, limit) => { ... },
    analyzeImage: async (question) => {     // â† NEU
        const visionEngine = getVisionEngine();
        if (!visionEngine.ready) throw new Error('Vision nicht geladen');
        const result = await visionEngine.askAboutImage(
            vision.uploadedImage!, question
        );
        return result.answer;
    },
});
```

#### 1.3 Bild-Kontext an LLM Ã¼bergeben âœ…

```typescript
// inference-engine.ts â€” generate() erweitern
// Nach RAG-Enrichment:
if (this.agentOrchestrator.context.imageContext) {
    const imgCtx = `\n\nğŸ–¼ï¸ **Aktuelles Bild:**\n${this.agentOrchestrator.context.imageContext}\n`;
    enrichedMessages = enrichedMessages.map(m =>
        m === lastUserMessage
            ? { ...m, content: imgCtx + m.content }
            : m
    );
}
```

---

### Phase 2: Multimodal Upgrade (Sprint 2) âœ… ABGESCHLOSSEN

#### 2.1 Unified Multimodal Pipeline âœ…

**Ziel:** Ein einziges VLM das Text + Bild versteht (statt separate Modelle)

```
AKTUELL:                          NEU (SOTA 2026):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phi-3.5 (Text)  â”‚              â”‚ Phi-3.5-vision (WebLLM) â”‚
â”‚ 3.8B via WebGPU â”‚              â”‚ 4.2B via WebGPU         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ Text + Bild unified     â”‚
         â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚ vit-gpt2 (Bild) â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 80MB via WASM   â”‚              â”‚ Florence-2 (Spezialist) â”‚
â”‚ âŒ Kein VQA     â”‚              â”‚ OCR, Detection, Ground  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ On-Demand Lazy-Load     â”‚
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Option A: WebLLM Phi-3.5-vision (Empfohlen fÃ¼r M4 MacBook)**
- Einziges Modell fÃ¼r Text UND Bild
- Nativ multimodal, kein Umweg Ã¼ber separate Vision-Engine
- ~4.2GB, passt in 16GB RAM mit WebGPU
- Braucht WebLLM Integration statt WebLLM + Transformers.js

**Option B: Dual-Model mit SmolVLM + Phi-3.5 (Konservativ)**
- Beibehaltung der aktuellen Architektur
- SmolVLM lazy-load bei Bild-Upload
- Phi-3.5 mini fÃ¼r Text-Chat
- Geringerer RAM-Verbrauch

#### 2.2 OCR als Killer-Feature âœ…

SmolVLM bietet VQA-basiertes OCR im Browser â€” kein Cloud-API nÃ¶tig:

```typescript
// Neue Capability: OCR aus Bildern
const ocrPipeline = await pipeline(
    'image-to-text',
    'Xenova/florence-2-base',
    { task: 'OCR', device: 'webgpu' }
);

const ocrResult = await ocrPipeline(imageUrl, {
    task: '<OCR>',
    max_new_tokens: 512
});
// â†’ Extrahierter Text aus dem Bild, direkt nutzbar
```

#### 2.3 Bild-zu-Code Pipeline âœ…

Ein Differenziator vs. Claude/ChatGPT: Bild â†’ Python-Analyse:

```
User lÃ¤dt Bild hoch (z.B. Diagramm, Tabelle)
    â†’ SmolVLM/Florence-2 analysiert Bild
    â†’ Agent erkennt: "Das ist ein Balkendiagramm mit Umsatzdaten"
    â†’ Agent schreibt automatisch Python-Code zur Reproduktion
    â†’ Pyodide fÃ¼hrt Code aus â†’ Matplotlib-Chart
    â†’ User bekommt interaktives Chart back!
```

---

### Phase 3: Inference-Upgrade (Sprint 3) âœ… ABGESCHLOSSEN

#### 3.1 Modell-Upgrade â€” SOTA 2026 âœ…

**Stand 2026:** Transformers.js v4 Preview ist verfÃ¼gbar mit:
- Neues C++ WebGPU Runtime (deutlich schneller)
- Ãœber 200 Modell-Architekturen
- MoE (Mixture of Experts) Support
- GPU-Operator-Optimierungen
- ~60 tok/s auf M4 Pro fÃ¼r GPT-OSS 20B (q4f16)

```json
// package.json â€” Upgrade
{
    "@huggingface/transformers": "^4.0.0-preview"
}
```

#### 3.2 Modell-Upgrade Pfade âœ…

| Modell | Parameter | Status | FÃ¤higkeiten |
|--------|-----------|--------|-------------|
| **Phi-3.5 mini** (legacy) | 3.8B | âœ… VerfÃ¼gbar | Text, CoT, Tools |
| **Phi-4 mini** (default) | 3.8B | âœ… **NEU** | Bestes Reasoning |
| **Qwen 2.5 1.5B** (balanced) | 1.5B | âœ… **NEU** | Bestes Deutsch, Code |
| **Phi-3.5-vision** (premium) | 4.2B | âœ… **NEU** | Multimodal VLM |
| **Llama 3.2 1B** (fast) | 1B | âœ… VerfÃ¼gbar | Ultraschnell |

**Empfehlung:**
- Default: **Qwen 2.5 3B** (bestes Deutsch + Code)
- Fallback Low-RAM: **Gemma 3 1B** (schnell, effizient)
- Premium: **Phi-4 mini** oder **Llama 3.2 3B**

#### 3.3 KV-Cache Reuse & Speculative Decoding

```typescript
// Neue Performance-Features
const config = {
    // KV-Cache Reuse: Spart ~40% bei Follow-up Fragen
    kvCacheReuse: true,
    
    // Speculative Decoding: Draft-Model (1B) + Verify-Model (3B)
    speculativeDecoding: {
        draftModel: 'Gemma-3-1B',
        verifyModel: 'Qwen-2.5-3B',
        // â†’ 2-3x Speedup bei gleicher QualitÃ¤t
    },
    
    // Continuous Batching fÃ¼r Multi-Turn
    continuousBatching: true,
};
```

---

### Phase 4: Competitive Parity (Sprint 4)

#### 4.1 MIMI vs. Claude/ChatGPT â€” Feature-Matrix

| Feature | Claude 3.5 | ChatGPT-4o | **MIMI (Aktuell)** | **MIMI (Ziel)** |
|---------|-----------|-----------|-------------------|----------------|
| Text Chat | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| Vision/Bild | â­â­â­â­â­ | â­â­â­â­â­ | â­ (kaputt) | â­â­â­â­ |
| Code Execution | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| RAG/Dokumente | â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| **PrivatsphÃ¤re** | âŒ Cloud | âŒ Cloud | âœ… **100% Lokal** | âœ… **100% Lokal** |
| **Offline** | âŒ | âŒ | âœ… | âœ… |
| **Kosten** | $20/mo | $20/mo | **GRATIS** | **GRATIS** |
| Web Search | â­â­â­â­â­ | â­â­â­â­ | â­â­ | â­â­â­ |
| Multi-Agent | âŒ | âŒ | â­â­â­ | â­â­â­â­ |
| Voice | â­â­â­ | â­â­â­â­â­ | â­â­ | â­â­â­ |

**MIMIs Killer-Features (USPs der keiner hat):**
1. **100% Lokal** â†’ Keine Daten verlassen den Browser
2. **Gratis** â†’ Keine Abo-Kosten
3. **Offline** â†’ Funktioniert ohne Internet
4. **Multi-Agent Swarm** â†’ 10+ Spezialisten-Agenten
5. **PWA** â†’ Installierbar wie App, kein App Store nÃ¶tig

#### 4.2 Neue Features fÃ¼r Competitive Edge

**4.2.1 Canvas / Artifacts (wie Claude)**
- Persistente Code-Artefakte die man bearbeiten kann
- Side-by-Side Editor + Chat
- Versionierung von generierten Dokumenten

**4.2.2 Computer Use / Screen Understanding**
- Screenshot-Analyse via Vision Engine
- DOM-Manipulation VorschlÃ¤ge
- UI-Debugging Hilfe

**4.2.3 MCP (Model Context Protocol) Integration**
- Anbindung an externe Tools (GitHub, Jira, etc.)
- Browser Extension als MCP Client
- Lokale Dateisystem-Integration via OPFS

**4.2.4 Agent-to-Agent Collaboration**
- Der Orchestrator koordiniert mehrere Agenten parallel
- Data Analyst â†’ Chart â†’ Creative Writer â†’ Report
- Automatische Delegation bei komplexen Aufgaben

---

## ğŸ¯ Quick-Fix: Vision sofort reparieren

### Schritt 1: `vision-engine.ts` â€” WebGPU + besseres Modell

```typescript
// Priority Fix: WebGPU aktivieren + SmolVLM laden
private async doInit(onProgress?: (status: string) => void): Promise<void> {
    const { pipeline, env } = await import('@huggingface/transformers');
    
    // WebGPU FIRST (30-100x schneller als WASM!)
    env.allowLocalModels = false;
    env.useBrowserCache = true;
    
    // Versuche WebGPU
    try {
        this.pipeline = await pipeline(
            'image-text-to-text',
            'HuggingFaceTB/SmolVLM-256M-Instruct', // Smallest but smart
            {
                device: 'webgpu',
                dtype: 'q4f16',
                progress_callback: (p) => {
                    if (p.status === 'progress' && p.progress) {
                        onProgress?.(`Lade Vision: ${Math.round(p.progress)}%`);
                    }
                }
            }
        );
        this.pipelineType = 'vlm';
    } catch (e) {
        // Fallback auf Florence-2 mit WASM
        this.pipeline = await pipeline(
            'image-to-text',
            'Xenova/florence-2-base-ft',
            { progress_callback: ... }
        );
        this.pipelineType = 'captioning';
    }
}
```

### Schritt 2: `useMimiVision.ts` â€” Ergebnis an Orchestrator Ã¼bergeben

```typescript
// Nach der Analyse: Bild-Kontext an Orchestrator
const orchestrator = getOrchestrator();
orchestrator.updateContext({
    imageContext: result.description
});
```

### Schritt 3: `useMimiEngine.ts` â€” analyzeImage Tool wiring

```typescript
engineRef.current.setToolContext({
    executePython: ...,
    searchDocuments: ...,
    analyzeImage: async (question: string) => {
        const visionEngine = getVisionEngine();
        if (!visionEngine.ready) {
            await visionEngine.init();
        }
        const result = await visionEngine.askAboutImage(
            vision.uploadedImage!, question
        );
        return result.answer;
    },
});
```

---

## ğŸ“… Timeline

| Phase | Timeframe | Aufwand | Status |
|-------|-----------|---------|-----------|
| **Phase 1: Vision Fix** | 1-2 Wochen | ~20h | âœ… FERTIG (10. Feb 2026) |
| **Phase 2: Multimodal** | 2-3 Wochen | ~40h | âœ… FERTIG (10. Feb 2026) |
| **Phase 3: Inference** | 2-4 Wochen | ~30h | âœ… FERTIG (10. Feb 2026) |
| **Phase 4: Competitive** | Ongoing | ~60h | ğŸŸ¢ In Arbeit |

---

## ğŸ”— Referenzen & SOTA 2026

- [Transformers.js v4 Preview](https://huggingface.co/docs/transformers.js) â€” Neues C++ WebGPU Runtime
- [WebLLM](https://github.com/nicedream1/web-llm) â€” OpenAI-kompatible API im Browser
- [SmolVLM](https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct) â€” Kompaktes VLM fÃ¼r Browser
- [Florence-2](https://huggingface.co/microsoft/Florence-2-base-ft) â€” OCR + Detection im Browser
- [WebGPU Cross-Browser Support](https://caniuse.com/webgpu) â€” Jan 2026: Chrome, Firefox, Safari, Edge
- [Phi-3.5-vision via WebLLM](https://huggingface.co/microsoft/Phi-3.5-vision-instruct) â€” Volles VLM

---

*Erstellt von MIMI Experten-Team Â· Feb 2026*
